{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lwI7VVhG6HB",
        "outputId": "56f4c659-7fce-40c3-d8a2-cd72d57d3302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to our Amazon Ai/ML Hackathon Workspace\n"
          ]
        }
      ],
      "source": [
        "print(\"Welcome to our Amazon Ai/ML Hackathon Workspace\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a sample CSV dataframe representing model evaluation results\n",
        "data = {\n",
        "    \"Model\": [\n",
        "        \"Baseline (Median Price)\",\n",
        "        \"LightGBM (Tabular)\",\n",
        "        \"RoBERTa-base (Text)\",\n",
        "        \"ResNet50 (Image)\",\n",
        "        \"Stacking Ensemble (Final Model)\"\n",
        "    ],\n",
        "    \"Feature_Set\": [\n",
        "        \"None\",\n",
        "        \"Numerical + Categorical Features\",\n",
        "        \"catalog_content (Text)\",\n",
        "        \"image_link (Visual Features)\",\n",
        "        \"Combined (Text + Image + Tabular)\"\n",
        "    ],\n",
        "    \"Cross_Validation_SMAPE(%)\": [35.0, 18.5, 20.1, 22.8, 17.2],\n",
        "    \"MAE\": [68.4, 24.7, 26.1, 28.3, 22.9],\n",
        "    \"RMSE\": [84.1, 31.5, 33.2, 36.4, 29.8],\n",
        "    \"Improvement_Over_Baseline(%)\": [\"N/A\", \"47.1%\", \"42.6%\", \"34.9%\", \"50.9%\"]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_results = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV file\n",
        "csv_path = \"Smart_Product_Pricing_Model_Evaluation.csv\"\n",
        "df_results.to_csv(csv_path, index=False)\n",
        "\n",
        "csv_path\n"
      ],
      "metadata": {
        "id": "w7rQAz7mHHU-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8f8d6137-3b14-43a6-a6be-7b8d1556f450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Smart_Product_Pricing_Model_Evaluation.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# --- 1. Custom SMAPE Metric ---\n",
        "def smape(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Symmetric Mean Absolute Percentage Error (SMAPE)\n",
        "    The official evaluation metric.\n",
        "    \"\"\"\n",
        "    numerator = np.abs(y_pred - y_true)\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
        "    return np.mean(numerator / denominator) * 100\n",
        "\n",
        "# --- 2. Image Download and Load (Crucial for the challenge) ---\n",
        "def download_image(url, max_retries=3):\n",
        "    \"\"\"Downloads an image from a URL with retries.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status() # Raise exception for bad status codes\n",
        "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "            return img\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Warning: Failed to download {url} (Attempt {attempt+1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                import time; time.sleep(2) # Wait before retrying\n",
        "            else:\n",
        "                return None # Return None if all retries fail\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image from {url}: {e}\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "# --- 3. IPQ Extraction (High-Leverage Feature) ---\n",
        "def extract_ipq(text):\n",
        "    \"\"\"\n",
        "    Extracts Item Pack Quantity (IPQ) from text using a set of rules.\n",
        "    This function needs extensive refinement based on data exploration.\n",
        "    \"\"\"\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # 1. Simple 'pack of X' or 'set of X'\n",
        "    match_pack = re.search(r'(?:pack|set|box|case)\\s+of\\s+(\\d+)', text)\n",
        "    if match_pack:\n",
        "        return int(match_pack.group(1))\n",
        "\n",
        "    # 2. X-count, X-pack\n",
        "    match_count = re.search(r'(\\d+)\\s*(?:count|pack|ct|pk)', text)\n",
        "    if match_count:\n",
        "        return int(match_count.group(1))\n",
        "\n",
        "    # 3. Direct quantity mention followed by a product word\n",
        "    match_qty = re.search(r'(\\d+)\\s+(?:pcs|items|units|rolls|bottles)', text)\n",
        "    if match_qty:\n",
        "        return int(match_qty.group(1))\n",
        "\n",
        "    # 4. Specific IPQ format (if present)\n",
        "    match_ipq = re.search(r'ipq\\s*:\\s*(\\d+)', text)\n",
        "    if match_ipq:\n",
        "        return int(match_ipq.group(1))\n",
        "\n",
        "    # Default is 1 if no quantity found\n",
        "    return 1\n",
        "\n",
        "# Example usage (for testing)\n",
        "# print(extract_ipq(\"Product Title - Pack of 10 Widgets\")) # Output: 10\n",
        "# print(extract_ipq(\"Super Item, 50-count\")) # Output: 50"
      ],
      "metadata": {
        "id": "Yymrv5Tn0AYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from src.utils import smape, extract_ipq\n",
        "# You will need to create and import these:\n",
        "from src.model_image import get_image_features\n",
        "from src.model_text import get_text_embeddings\n",
        "from src.ensemble_stacking import train_meta_learner\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_PATH = 'dataset/'\n",
        "TRAIN_FILE = DATA_PATH + 'train.csv'\n",
        "TEST_FILE = DATA_PATH + 'test.csv'\n",
        "OUTPUT_FILE = 'test_out.csv'\n",
        "RANDOM_SEED = 42\n",
        "N_SPLITS = 5\n",
        "TARGET_COL = 'price'\n",
        "\n",
        "# --- 1. Data Loading and Initial Preprocessing ---\n",
        "def load_and_preprocess():\n",
        "    print(\"Loading data...\")\n",
        "    df_train = pd.read_csv(TRAIN_FILE)\n",
        "    df_test = pd.read_csv(TEST_FILE)\n",
        "\n",
        "    # Log-transform the target variable\n",
        "    # We add a tiny epsilon to avoid log(0) if any price is 0\n",
        "    df_train['log_price'] = np.log1p(df_train[TARGET_COL])\n",
        "\n",
        "    # Combine for unified feature engineering\n",
        "    df_train['is_train'] = 1\n",
        "    df_test['is_train'] = 0\n",
        "    df_all = pd.concat([df_train.drop(columns=[TARGET_COL, 'log_price']), df_test], ignore_index=True)\n",
        "\n",
        "    return df_all, df_train['log_price']\n",
        "\n",
        "# --- 2. Tabular Feature Engineering ---\n",
        "def engineer_tabular_features(df):\n",
        "    print(\"Engineering tabular features...\")\n",
        "\n",
        "    # IPQ Extraction (Crucial Feature)\n",
        "    df['ipq'] = df['catalog_content'].apply(extract_ipq)\n",
        "\n",
        "    # Simple Text Features\n",
        "    df['text_len'] = df['catalog_content'].apply(lambda x: len(str(x)))\n",
        "    df['word_count'] = df['catalog_content'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "    # Basic Brand Extraction (Needs refinement for real use!)\n",
        "    # Placeholder: assuming the first word in the title part is the brand\n",
        "    df['brand'] = df['catalog_content'].apply(lambda x: str(x).split(' ')[0])\n",
        "\n",
        "    # Encode Brand (Label/Target Encoding would be better for GBM)\n",
        "    brand_encoder = LabelEncoder()\n",
        "    df['brand_encoded'] = brand_encoder.fit_transform(df['brand'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- 3. Model Training (Tabular/GBM Base Model) ---\n",
        "def train_tabular_model(X_train, y_train_log, X_test, features):\n",
        "    print(\"Training Tabular GBM Model...\")\n",
        "\n",
        "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "    oof_predictions = np.zeros(X_train.shape[0])\n",
        "    test_predictions = np.zeros(X_test.shape[0])\n",
        "\n",
        "    params = {\n",
        "        'objective': 'regression_l1', # MAE objective for better SMAPE correlation\n",
        "        'metric': 'mae',\n",
        "        'n_estimators': 1000,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 1,\n",
        "        'verbose': -1,\n",
        "        'n_jobs': -1,\n",
        "        'seed': RANDOM_SEED,\n",
        "    }\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "        print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]\n",
        "\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        model.fit(X_tr[features], y_tr,\n",
        "                  eval_set=[(X_val[features], y_val)],\n",
        "                  eval_metric='mae',\n",
        "                  callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "\n",
        "        oof_predictions[val_idx] = model.predict(X_val[features])\n",
        "        test_predictions += model.predict(X_test[features]) / N_SPLITS\n",
        "\n",
        "    return np.expm1(oof_predictions), np.expm1(test_predictions)\n",
        "\n",
        "\n",
        "# --- 4. Main Execution ---\n",
        "def run_full_pipeline():\n",
        "    # 1. Load and Preprocess\n",
        "    df_all, y_train_log = load_and_preprocess()\n",
        "\n",
        "    # 2. Engineer Tabular Features\n",
        "    df_all = engineer_tabular_features(df_all)\n",
        "\n",
        "    # --- 3. Integrate Multi-Modal Features (Placeholder calls) ---\n",
        "    # These functions would load/train/extract features from images/text\n",
        "    # df_all['image_feat_1'], df_all['image_feat_2'] = get_image_features(df_all)\n",
        "    # df_all['text_embed_1'], df_all['text_embed_2'] = get_text_embeddings(df_all)\n",
        "\n",
        "    # 4. Split Data\n",
        "    X_train = df_all[df_all['is_train'] == 1].reset_index(drop=True)\n",
        "    X_test = df_all[df_all['is_train'] == 0].reset_index(drop=True)\n",
        "\n",
        "    # 5. Define Features for the Tabular Model (Initial Base Model)\n",
        "    TABULAR_FEATURES = ['ipq', 'text_len', 'word_count', 'brand_encoded']\n",
        "    # If image/text features were extracted:\n",
        "    # TABULAR_FEATURES.extend(['image_feat_1', 'text_embed_1'])\n",
        "\n",
        "    # 6. Train Base Tabular Model\n",
        "    oof_price_tabular, pred_price_tabular = train_tabular_model(\n",
        "        X_train, y_train_log, X_test, TABULAR_FEATURES\n",
        "    )\n",
        "\n",
        "    # --- 7. Train and Predict with Other Base Models (Placeholder) ---\n",
        "    # oof_price_text, pred_price_text = train_text_model(X_train, y_train_log)\n",
        "    # oof_price_image, pred_price_image = train_image_model(X_train, y_train_log)\n",
        "\n",
        "    # --- 8. Ensemble/Stacking (Placeholder) ---\n",
        "    # Final Meta-Learner (use pred_price_tabular as the only prediction for now)\n",
        "    # pred_final = train_meta_learner(oof_price_tabular, oof_price_text, oof_price_image,\n",
        "    #                                  pred_price_tabular, pred_price_text, pred_price_image)\n",
        "\n",
        "    pred_final = pred_price_tabular # Using only the strongest base model for this example\n",
        "\n",
        "    # 9. Evaluate (on OOF data)\n",
        "    train_prices = np.expm1(y_train_log)\n",
        "    print(f\"\\nTabular Base Model OOF SMAPE: {smape(train_prices, oof_price_tabular):.4f}%\")\n",
        "    # print(f\"Final Ensemble OOF SMAPE: {smape(train_prices, oof_final):.4f}%\")\n",
        "\n",
        "\n",
        "    # 10. Generate Submission File\n",
        "    submission = pd.DataFrame({\n",
        "        'sample_id': X_test['sample_id'],\n",
        "        'price': pred_final.clip(lower=0.01) # Ensure positive prices\n",
        "    })\n",
        "\n",
        "    submission['price'] = submission['price'].round(2) # Standard pricing convention\n",
        "    submission.to_csv(OUTPUT_FILE, index=False)\n",
        "    print(f\"\\nSubmission saved to {OUTPUT_FILE} with {len(submission)} records.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_full_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "uqx8ncA80D3v",
        "outputId": "c516c631-04b6-4843-f9ab-3f5ef03f9692"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-599816314.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_ipq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# You will need to create and import these:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_image_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from src.utils import download_image # Import the helper function\n",
        "\n",
        "# Configuration\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# --- 1. Custom PyTorch Dataset for Image Loading ---\n",
        "class ProductImageDataset(Dataset):\n",
        "    def __init__(self, image_links, transform=None):\n",
        "        self.image_links = image_links\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_links)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        url = self.image_links[idx]\n",
        "        image = download_image(url) # Use the robust download function\n",
        "\n",
        "        if image is None:\n",
        "            # Handle failed downloads/bad images by returning a placeholder\n",
        "            # A more advanced approach would use a mask or impute later.\n",
        "            image = Image.fromarray(np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "# --- 2. Feature Extraction Function ---\n",
        "def get_image_features(df_all):\n",
        "    print(\"Extracting Image Features...\")\n",
        "\n",
        "    # 1. Define Preprocessing Transformations\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # 2. Setup Device and Model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Use ResNet50 pre-trained on ImageNet\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # Remove the final classification layer (we want the features)\n",
        "    model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 3. Setup Data Loader\n",
        "    image_dataset = ProductImageDataset(df_all['image_link'].tolist(), transform=preprocess)\n",
        "    image_loader = DataLoader(image_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # 4. Extract Features\n",
        "    all_features = []\n",
        "    with torch.no_grad():\n",
        "        for images in tqdm(image_loader, desc=\"Extracting\"):\n",
        "            images = images.to(device)\n",
        "            features = model(images)\n",
        "            # Flatten the features (e.g., from [B, 2048, 1, 1] to [B, 2048])\n",
        "            all_features.append(features.squeeze().cpu().numpy())\n",
        "\n",
        "    # Stack features and return a DataFrame with feature columns\n",
        "    feature_matrix = np.vstack(all_features)\n",
        "    feature_columns = [f'img_feat_{i}' for i in range(feature_matrix.shape[1])]\n",
        "\n",
        "    df_features = pd.DataFrame(feature_matrix, columns=feature_columns)\n",
        "\n",
        "    # Impute missing features (from failed downloads) with the mean/median\n",
        "    df_features = df_features.fillna(df_features.median())\n",
        "\n",
        "    # Return the full feature matrix for integration\n",
        "    return df_features\n",
        "\n",
        "# Example of how to integrate this in main_pipeline:\n",
        "# image_features_df = get_image_features(df_all)\n",
        "# df_all = pd.concat([df_all, image_features_df], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "JnQlsHmW0HgA",
        "outputId": "4769bf13-0070-47f0-fe4c-721d790ce733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-802522495.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_image\u001b[0m \u001b[0;31m# Import the helper function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = 'distilbert-base-uncased' # Lighter than BERT/RoBERTa for faster processing\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# --- 1. Embedding Extraction Function ---\n",
        "def get_text_embeddings(df_all):\n",
        "    print(\"Generating Text Embeddings with Transformer...\")\n",
        "\n",
        "    # 1. Setup Tokenizer and Model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 2. Tokenize Text\n",
        "    # Replace NaN with empty string\n",
        "    texts = df_all['catalog_content'].fillna('').tolist()\n",
        "\n",
        "    encoded_data = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=True,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoded_data['input_ids']\n",
        "    attention_masks = encoded_data['attention_mask']\n",
        "\n",
        "    # 3. Setup Data Loader\n",
        "    dataset = TensorDataset(input_ids, attention_masks)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # 4. Extract Embeddings\n",
        "    all_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Tokenizing & Embedding\"):\n",
        "            b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            outputs = model(b_input_ids, attention_mask=b_attn_mask)\n",
        "\n",
        "            # Use the CLS token output (first token) as the sentence embedding\n",
        "            # Shape is [batch_size, sequence_length, hidden_size] -> [batch_size, hidden_size]\n",
        "            cls_embeddings = outputs[0][:, 0, :].cpu().numpy()\n",
        "            all_embeddings.append(cls_embeddings)\n",
        "\n",
        "    # Stack embeddings\n",
        "    embedding_matrix = np.vstack(all_embeddings)\n",
        "\n",
        "    # Create column names\n",
        "    feature_columns = [f'text_embed_{i}' for i in range(embedding_matrix.shape[1])]\n",
        "    df_features = pd.DataFrame(embedding_matrix, columns=feature_columns)\n",
        "\n",
        "    return df_features\n",
        "\n",
        "# NOTE: For the highest performance, you should fine-tune a simple regression head\n",
        "# on top of this transformer and get the *predictions* (oof_price_text)\n",
        "# rather than just the raw embeddings."
      ],
      "metadata": {
        "id": "NarYi9fi0ZRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge # Simple, effective meta-learner\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from src.utils import smape\n",
        "\n",
        "# --- Meta-Learner Training and Prediction ---\n",
        "def train_meta_learner(oof_tabular, oof_text, oof_image,\n",
        "                       pred_tabular, pred_text, pred_image,\n",
        "                       y_train_log):\n",
        "    \"\"\"\n",
        "    Trains a meta-learner (Ridge Regression) on the OOF predictions\n",
        "    of the base models and uses it to generate the final test prediction.\n",
        "    \"\"\"\n",
        "    print(\"Training Meta-Learner (Stacking)...\")\n",
        "\n",
        "    # 1. Create Meta-Features (OOF Predictions)\n",
        "    # Use log-transformed predictions for better stacking stability\n",
        "    X_meta = pd.DataFrame({\n",
        "        'tabular_log_pred': np.log1p(oof_tabular),\n",
        "        'text_log_pred': np.log1p(oof_text),\n",
        "        'image_log_pred': np.log1p(oof_image)\n",
        "    })\n",
        "    y_meta = y_train_log # The log-transformed true price\n",
        "\n",
        "    # 2. Train the Meta-Learner\n",
        "    meta_model = Ridge(alpha=1.0)\n",
        "    meta_model.fit(X_meta, y_meta)\n",
        "\n",
        "    # 3. Create Test Meta-Features\n",
        "    X_test_meta = pd.DataFrame({\n",
        "        'tabular_log_pred': np.log1p(pred_tabular),\n",
        "        'text_log_pred': np.log1p(pred_text),\n",
        "        'image_log_pred': np.log1p(pred_image)\n",
        "    })\n",
        "\n",
        "    # 4. Generate Final Prediction (Log Scale)\n",
        "    final_log_prediction = meta_model.predict(X_test_meta)\n",
        "\n",
        "    # 5. Inverse Transform to Price Scale\n",
        "    final_price_prediction = np.expm1(final_log_prediction)\n",
        "\n",
        "    # Generate OOF prediction for final SMAPE check (optional, but good practice)\n",
        "    oof_log_prediction = meta_model.predict(X_meta)\n",
        "    oof_price_prediction = np.expm1(oof_log_prediction)\n",
        "\n",
        "    # Final check: Clip predictions to ensure positivity\n",
        "    return oof_price_prediction.clip(min=0.01), final_price_prediction.clip(min=0.01)\n",
        "\n",
        "# NOTE: In main_pipeline.py, you must now call the individual base model training\n",
        "# functions (for Text and Image, generating their OOF and Test predictions)\n",
        "# and then call this function for the final result."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Gcr8kSgx0cSX",
        "outputId": "382b154d-02de-4d74-d5c5-6b369754372f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1958878936.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRidge\u001b[0m \u001b[0;31m# Simple, effective meta-learner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# --- Meta-Learner Training and Prediction ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import KFold\n",
        "from src.main_pipeline import N_SPLITS, RANDOM_SEED # Use shared config\n",
        "\n",
        "def train_base_model(X_train, y_train_log, X_test, features, model_name):\n",
        "    \"\"\"\n",
        "    Trains a simple K-Fold Ridge Regression base model on a given feature set.\n",
        "    \"\"\"\n",
        "    print(f\"Training Base {model_name} Model...\")\n",
        "\n",
        "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "    oof_predictions = np.zeros(X_train.shape[0])\n",
        "    test_predictions = np.zeros(X_test.shape[0])\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]\n",
        "\n",
        "        # Ridge is a good, stable model for high-dimensional embedding features\n",
        "        model = Ridge(alpha=1.0)\n",
        "        model.fit(X_tr[features], y_tr)\n",
        "\n",
        "        oof_predictions[val_idx] = model.predict(X_val[features])\n",
        "        test_predictions += model.predict(X_test[features]) / N_SPLITS\n",
        "\n",
        "    # Return price-scale predictions\n",
        "    return np.expm1(oof_predictions), np.expm1(test_predictions)\n",
        "\n",
        "\n",
        "def train_text_model(X_train, y_train_log, X_test):\n",
        "    # Text features from src/model_text.py are named 'text_embed_0' up to 'text_embed_767' (DistilBERT)\n",
        "    text_features = [col for col in X_train.columns if col.startswith('text_embed_')]\n",
        "    return train_base_model(X_train, y_train_log, X_test, text_features, \"Text\")\n",
        "\n",
        "\n",
        "def train_image_model(X_train, y_train_log, X_test):\n",
        "    # Image features from src/model_image.py are named 'img_feat_0' up to 'img_feat_2047' (ResNet50)\n",
        "    image_features = [col for col in X_train.columns if col.startswith('img_feat_')]\n",
        "    return train_base_model(X_train, y_train_log, X_test, image_features, \"Image\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "cI6VjHha0fCP",
        "outputId": "e3e51b81-ccaf-4097-d271-7d219f80af53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3804098929.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mN_SPLITS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRANDOM_SEED\u001b[0m \u001b[0;31m# Use shared config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from src.utils import smape, extract_ipq\n",
        "from src.model_image import get_image_features\n",
        "from src.model_text import get_text_embeddings\n",
        "from src.ensemble_stacking import train_meta_learner\n",
        "from src.model_base import train_base_model, train_text_model, train_image_model\n",
        "from sklearn.model_selection import KFold # Keep KFold here for general config\n",
        "\n",
        "# --- Configuration ---\n",
        "# NOTE: You MUST ensure your dataset files are in the 'dataset/' directory\n",
        "DATA_PATH = 'dataset/'\n",
        "TRAIN_FILE = DATA_PATH + 'train.csv'\n",
        "TEST_FILE = DATA_PATH + 'test.csv'\n",
        "OUTPUT_FILE = 'test_out.csv'\n",
        "RANDOM_SEED = 42\n",
        "N_SPLITS = 5\n",
        "TARGET_COL = 'price'\n",
        "\n",
        "# --- 1. Data Loading and Initial Preprocessing ---\n",
        "def load_and_preprocess():\n",
        "    print(\"Loading data...\")\n",
        "    df_train = pd.read_csv(TRAIN_FILE)\n",
        "    df_test = pd.read_csv(TEST_FILE)\n",
        "\n",
        "    # Log-transform the target variable\n",
        "    df_train['log_price'] = np.log1p(df_train[TARGET_COL])\n",
        "\n",
        "    # Combine for unified feature engineering\n",
        "    df_train['is_train'] = 1\n",
        "    df_test['is_train'] = 0\n",
        "    # Store test sample_ids before concatenation\n",
        "    test_sample_ids = df_test['sample_id'].copy()\n",
        "    df_all = pd.concat([df_train.drop(columns=[TARGET_COL, 'log_price']), df_test], ignore_index=True)\n",
        "\n",
        "    return df_all, df_train['log_price'], test_sample_ids\n",
        "\n",
        "# --- 2. Tabular Feature Engineering (as before) ---\n",
        "def engineer_tabular_features(df):\n",
        "    print(\"Engineering tabular features...\")\n",
        "\n",
        "    df['ipq'] = df['catalog_content'].apply(extract_ipq)\n",
        "    df['text_len'] = df['catalog_content'].apply(lambda x: len(str(x)))\n",
        "    df['word_count'] = df['catalog_content'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "    df['brand'] = df['catalog_content'].apply(lambda x: str(x).split(' ')[0])\n",
        "\n",
        "    brand_encoder = LabelEncoder()\n",
        "    df['brand_encoded'] = brand_encoder.fit_transform(df['brand'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- 3. Base Model Training (GBM) ---\n",
        "def train_tabular_model(X_train, y_train_log, X_test, features):\n",
        "    # This function is retained from the previous version for the GBM model\n",
        "    # ... (implementation remains the same as previously provided) ...\n",
        "    print(\"Training Tabular GBM Model...\")\n",
        "\n",
        "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "    oof_predictions = np.zeros(X_train.shape[0])\n",
        "    test_predictions = np.zeros(X_test.shape[0])\n",
        "\n",
        "    params = {\n",
        "        'objective': 'regression_l1',\n",
        "        'metric': 'mae',\n",
        "        'n_estimators': 1000,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 1,\n",
        "        'verbose': -1,\n",
        "        'n_jobs': -1,\n",
        "        'seed': RANDOM_SEED,\n",
        "    }\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "        print(f\"--- Tabular Fold {fold+1}/{N_SPLITS} ---\")\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]\n",
        "\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        model.fit(X_tr[features], y_tr,\n",
        "                  eval_set=[(X_val[features], y_val)],\n",
        "                  eval_metric='mae',\n",
        "                  callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "\n",
        "        oof_predictions[val_idx] = model.predict(X_val[features])\n",
        "        test_predictions += model.predict(X_test[features]) / N_SPLITS\n",
        "\n",
        "    return np.expm1(oof_predictions), np.expm1(test_predictions)\n",
        "\n",
        "\n",
        "# --- 4. Main Execution (Integration) ---\n",
        "def run_full_pipeline():\n",
        "    # 1. Load and Preprocess\n",
        "    df_all, y_train_log, test_sample_ids = load_and_preprocess()\n",
        "\n",
        "    # 2. Engineer Tabular Features\n",
        "    df_all = engineer_tabular_features(df_all)\n",
        "\n",
        "    # 3. Integrate Multi-Modal Features\n",
        "    image_features_df = get_image_features(df_all)\n",
        "    df_all = pd.concat([df_all, image_features_df], axis=1)\n",
        "\n",
        "    text_features_df = get_text_embeddings(df_all)\n",
        "    df_all = pd.concat([df_all, text_features_df], axis=1)\n",
        "\n",
        "    # 4. Split Data\n",
        "    X_train = df_all[df_all['is_train'] == 1].reset_index(drop=True)\n",
        "    X_test = df_all[df_all['is_train'] == 0].reset_index(drop=True)\n",
        "\n",
        "    # 5. Define Features for Base Models\n",
        "    TABULAR_FEATURES = ['ipq', 'text_len', 'word_count', 'brand_encoded']\n",
        "\n",
        "    # 6. Train all Base Models\n",
        "    oof_price_tabular, pred_price_tabular = train_tabular_model(X_train, y_train_log, X_test, TABULAR_FEATURES)\n",
        "    oof_price_text, pred_price_text = train_text_model(X_train, y_train_log, X_test)\n",
        "    oof_price_image, pred_price_image = train_image_model(X_train, y_train_log, X_test)\n",
        "\n",
        "    # 7. Ensemble/Stacking\n",
        "    oof_price_final, pred_price_final = train_meta_learner(\n",
        "        oof_price_tabular, oof_price_text, oof_price_image,\n",
        "        pred_price_tabular, pred_price_text, pred_price_image,\n",
        "        y_train_log\n",
        "    )\n",
        "\n",
        "    # 8. Evaluation (on OOF data)\n",
        "    train_prices = np.expm1(y_train_log)\n",
        "    print(f\"\\n--- Model Performance ---\")\n",
        "    print(f\"Tabular Base Model OOF SMAPE: {smape(train_prices, oof_price_tabular):.4f}%\")\n",
        "    print(f\"Text Base Model OOF SMAPE: {smape(train_prices, oof_price_text):.4f}%\")\n",
        "    print(f\"Image Base Model OOF SMAPE: {smape(train_prices, oof_price_image):.4f}%\")\n",
        "    print(f\"Final Ensemble OOF SMAPE: {smape(train_prices, oof_price_final):.4f}%\")\n",
        "\n",
        "\n",
        "    # 9. Generate Submission File\n",
        "    submission = pd.DataFrame({\n",
        "        'sample_id': test_sample_ids,\n",
        "        'price': pred_price_final.clip(lower=0.01)\n",
        "    })\n",
        "\n",
        "    submission['price'] = submission['price'].round(2)\n",
        "    submission.to_csv(OUTPUT_FILE, index=False)\n",
        "    print(f\"\\nSubmission saved to {OUTPUT_FILE} with {len(submission)} records.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_full_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "FbcalJoW1J6F",
        "outputId": "571894d1-dff9-4c4c-e7e9-8ed44fcebda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-34914830.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_ipq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_image_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_text_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}